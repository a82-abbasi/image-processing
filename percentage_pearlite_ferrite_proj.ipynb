{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPX2FI285tFWxR09mV/R0KM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a82-abbasi/image-processing/blob/main/percentage_pearlite_ferrite_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnNW_4SHE_nk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjPyWb_hNPvV",
        "outputId": "a7c78c73-9486-4be0-9908-97c473c751ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/examples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwsdEDOeNjvt",
        "outputId": "5c8bef5d-3631-48e0-ec67-e37704f73fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRVgwAwSN125",
        "outputId": "f6f15c7b-943f-4c8e-bf77-f72a447debe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/seg_mod_1\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/seg_mod_1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsMUmcxSOHST"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from pathlib import Path\n",
        "import re\n",
        "from skimage import measure\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
        "import matplotlib as mpl\n",
        "import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import normalize\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acByb69PFSeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bc8hINGOhz9"
      },
      "outputs": [],
      "source": [
        "\n",
        "images_path = \"./train/micro_project/img_256\"\n",
        "masks_path = \"./train/micro_project/img_msmask256\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xwOVSQEXRbg"
      },
      "outputs": [],
      "source": [
        "def standardize(x):\n",
        "    x = np.array(x, dtype='float64')\n",
        "    x -= np.min(x)\n",
        "    x /= np.percentile(x, 98)\n",
        "    x[x > 1] = 1\n",
        "    return x\n",
        "\n",
        "def preprocessing(img):\n",
        "    image = np.array(img)\n",
        "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "    image = np.zeros_like(image)\n",
        "    image[:,:,0] = gray\n",
        "    image[:,:,1] = gray\n",
        "    image[:,:,2] = gray\n",
        "    image = standardize(image)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3OHPCwwQkyO"
      },
      "outputs": [],
      "source": [
        "images_paths = []\n",
        "masks_paths = []\n",
        "\n",
        "for imgname in os.listdir(images_path):\n",
        "  images_paths.append(os.path.join(images_path,imgname))\n",
        "\n",
        "for imgname in os.listdir(masks_path):\n",
        "  masks_paths.append(os.path.join(masks_path,imgname))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1QscuzjFlpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiG-yhlHTYcl"
      },
      "outputs": [],
      "source": [
        "images_paths.sort()\n",
        "masks_paths.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Xc5Af3X4KE",
        "outputId": "89c7110a-8ff6-49e0-b2da-31e1537ee59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 504/504 [00:21<00:00, 23.65it/s]\n",
            "100%|██████████| 504/504 [07:15<00:00,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class values:  [0 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#Capture training image info as a list\n",
        "train_images = []\n",
        "train_masks = []\n",
        "\n",
        "for imgpath in tqdm.tqdm(images_paths):\n",
        "    img = cv2.imread(imgpath)\n",
        "    img = preprocessing(img)\n",
        "    train_images.append(img)\n",
        "\n",
        "\n",
        "for maskpath in tqdm.tqdm(masks_paths):\n",
        "     mask0 = cv2.imread(maskpath, 0)\n",
        "     mask0 = np.where(mask0 > 125, 1, 0)\n",
        "     train_masks.append(mask0)\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "train_masks = np.array(train_masks)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_images, train_masks, test_size = 0.10, shuffle=True, random_state = 1)\n",
        "print(\"Class values: \", np.unique(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_gSG_QhWMI4",
        "outputId": "afb86846-6f06-4ce0-f905-34653f79cf3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (453, 256, 256)\n",
            "y_train shape: (51, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "# Print shapes of arrays\n",
        "print(\"x_train shape:\", y_train.shape)\n",
        "print(\"y_train shape:\", y_val.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po6UE77WVyPF"
      },
      "outputs": [],
      "source": [
        "y_train=np.expand_dims(np.array(y_train), axis=3)\n",
        "y_val=np.expand_dims(np.array(y_val), axis=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NORM = mpl.colors.Normalize(vmin=0, vmax=2)\n",
        "\n",
        "plt.figure(figsize=(16,10))\n",
        "for i in range(1,4):\n",
        "    plt.subplot(2,3,i)\n",
        "    img = X_val[i]\n",
        "    plt.imshow(img)\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "\n",
        "for i in range(4,7):\n",
        "    plt.subplot(2,3,i)\n",
        "    img = np.squeeze(y_val[i-3])\n",
        "    plt.imshow(img, cmap='jet', norm=NORM)\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v95-x_Wp3WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Concatenate, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# تابع اصلاح‌شده برای IOU (برای segmentation باینری)\n",
        "def binary_iou(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)  # تبدیل y_true به float32\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)  # threshold و تبدیل به float32\n",
        "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = tf.reduce_sum(y_true + y_pred, axis=[1, 2, 3]) - intersection\n",
        "    iou = (intersection + 1e-7) / (union + 1e-7)  # جلوگیری از تقسیم بر صفر\n",
        "    return tf.reduce_mean(iou)\n",
        "\n",
        "# تابع برای محاسبه IOU تک تصویر در show_predictions\n",
        "def single_image_iou(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)  # تبدیل y_true به float32\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true + y_pred) - intersection\n",
        "    return (intersection + 1e-7) / (union + 1e-7)\n",
        "\n",
        "def unet_model(output_channels):\n",
        "    IMG_HEIGHT = X_train.shape[1]\n",
        "    IMG_WIDTH  = X_train.shape[2]\n",
        "    IMG_CHANNELS = X_train.shape[3]\n",
        "\n",
        "    base_model = MobileNetV2(input_shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS], include_top=False)\n",
        "\n",
        "    layer_names = [\n",
        "        'block_1_expand_relu',   # 64x64\n",
        "        'block_3_expand_relu',   # 32x32\n",
        "        'block_6_expand_relu',   # 16x16\n",
        "        'block_13_expand_relu',  # 8x8\n",
        "        'block_16_project',      # 4x4\n",
        "    ]\n",
        "\n",
        "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "    down_stack = Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "    down_stack.trainable = False\n",
        "\n",
        "    up_stack = [\n",
        "        pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "        pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "        pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "        pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "    ]\n",
        "\n",
        "    inputs = Input(shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
        "    skips = down_stack(inputs)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        concat = Concatenate()\n",
        "        x = concat([x, skip])\n",
        "\n",
        "    last = Conv2DTranspose(output_channels, 3, strides=2, padding='same', activation='sigmoid')\n",
        "    x = last(x)\n",
        "    return Model(inputs=inputs, outputs=x)\n",
        "\n",
        "def create_mask(pred_mask):\n",
        "    pred_mask = tf.sigmoid(pred_mask)\n",
        "    pred_mask = pred_mask[..., tf.newaxis]\n",
        "    return pred_mask[0]\n",
        "\n",
        "def show_predictions(epoch, dataset=None, num=50):\n",
        "    if dataset:\n",
        "        for image, mask in dataset.take(num):\n",
        "            pred_mask = model.predict(image)\n",
        "            iou_score = single_image_iou(mask[0], create_mask(pred_mask))\n",
        "            plt.figure(figsize=(15, 10))\n",
        "            plt.subplot(231)\n",
        "            plt.title('Testing Image')\n",
        "            plt.imshow(image[0], cmap='gray')\n",
        "            plt.subplot(232)\n",
        "            plt.title('Ground Truth')\n",
        "            plt.imshow(mask[0], cmap='gray')\n",
        "            plt.subplot(233)\n",
        "            plt.title(f'Prediction on test image\\nIOU: {iou_score:.4f}')\n",
        "            plt.imshow(create_mask(pred_mask), cmap='gray')\n",
        "\n",
        "            plt.savefig(f\"results/mask_{str(ii)}.png\")\n",
        "            plt.show()\n",
        "            print(f'IOU Score for this image: {iou_score:.4f}')\n",
        "    else:\n",
        "        fig = plt.figure(figsize=(12, 12))\n",
        "        fig.suptitle(f\"\\n Epoch: {str(epoch)}\\n\", fontsize=16)\n",
        "\n",
        "        pred_mask = model.predict(train_images[num][tf.newaxis, ...])\n",
        "        iou_score = single_image_iou(train_masks[num], create_mask(pred_mask))\n",
        "        plt.subplot(331)\n",
        "        plt.title('Testing Image')\n",
        "        plt.imshow(train_images[num], cmap='gray')\n",
        "        plt.subplot(332)\n",
        "        plt.title('Ground Truth')\n",
        "        plt.imshow(train_masks[num], cmap='gray')\n",
        "        plt.subplot(333)\n",
        "        plt.title(f'Prediction on test image\\nIOU: {iou_score:.4f}')\n",
        "        plt.imshow(create_mask(pred_mask)[:,:,0], cmap='gray')\n",
        "\n",
        "        pred_mask = model.predict(train_images[num+16][tf.newaxis, ...])\n",
        "        iou_score = single_image_iou(train_masks[num+16], create_mask(pred_mask))\n",
        "        plt.subplot(334)\n",
        "        plt.imshow(train_images[num+16], cmap='gray')\n",
        "        plt.subplot(335)\n",
        "        plt.imshow(train_masks[num+16], cmap='gray')\n",
        "        plt.subplot(336)\n",
        "        plt.title(f'Prediction on test image\\nIOU: {iou_score:.4f}')\n",
        "        plt.imshow(create_mask(pred_mask)[:,:,0], cmap='gray')\n",
        "\n",
        "        pred_mask = model.predict(train_images[num+14][tf.newaxis, ...])\n",
        "        iou_score = single_image_iou(train_masks[num+14], create_mask(pred_mask))\n",
        "        plt.subplot(337)\n",
        "        plt.imshow(train_images[num+14], cmap='gray')\n",
        "        plt.subplot(338)\n",
        "        plt.imshow(train_masks[num+14], cmap='gray')\n",
        "        plt.subplot(339)\n",
        "        plt.title(f'Prediction on test image\\nIOU: {iou_score:.4f}')\n",
        "        plt.imshow(create_mask(pred_mask)[:,:,0], cmap='gray')\n",
        "\n",
        "        plt.show()\n",
        "        print(f'IOU Scores for displayed images: {iou_score:.4f} (last image)')\n",
        "\n",
        "output_channels = 1\n",
        "model = unet_model(output_channels)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', binary_iou])\n",
        "\n",
        "EPOCHS = 50\n",
        "VAL_SUBSPLITS = 5\n",
        "BATCH_SIZE = 16\n",
        "VALIDATION_STEPS = len(X_val)//BATCH_SIZE//VAL_SUBSPLITS\n",
        "STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\n",
        "\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        show_predictions(epoch)\n",
        "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
        "\n",
        "model_history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         verbose=1,\n",
        "                         validation_data=(X_val, y_val),\n",
        "                         callbacks=[DisplayCallback()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "xMiOKAQwHB4V",
        "outputId": "20e3652e-53d7-4836-e603-20ed4aa61384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3968630675.py:29: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS], include_top=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3968630675.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'\\nSample Prediction after epoch {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m model_history = model.fit(X_train, y_train, epochs=EPOCHS,\n\u001b[0m\u001b[1;32m    143\u001b[0m                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3968630675.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDisplayCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mshow_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'\\nSample Prediction after epoch {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3968630675.py\u001b[0m in \u001b[0;36mshow_predictions\u001b[0;34m(epoch, dataset, num)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mpred_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0miou_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_image_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m331\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing Image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3968630675.py\u001b[0m in \u001b[0;36msingle_image_iou\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# تبدیل y_true به float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mintersection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0munion\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_metric(y_true, y_pred):\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.float32)  # threshold 0.5\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    return intersection / (union + 1e-7)\n"
      ],
      "metadata": {
        "id": "JxrMT2CXJb20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model1(output_channels):\n",
        "    IMG_HEIGHT = X_train.shape[1]\n",
        "    IMG_WIDTH  = X_train.shape[2]\n",
        "    IMG_CHANNELS = X_train.shape[3]\n",
        "\n",
        "    base_model = MobileNetV2(input_shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS], include_top=False, weights='imagenet')\n",
        "\n",
        "    # Use the activations of these layers\n",
        "    layer_names = [\n",
        "        'block_1_expand_relu',   # 64x64\n",
        "        'block_3_expand_relu',   # 32x32\n",
        "        'block_6_expand_relu',   # 16x16\n",
        "        'block_13_expand_relu',  # 8x8\n",
        "        'block_16_project',      # 4x4\n",
        "    ]\n",
        "\n",
        "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "    # Create the feature extraction model\n",
        "    down_stack = Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "    down_stack.trainable = False\n",
        "\n",
        "    up_stack = [\n",
        "        pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "        pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "        pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "        pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "    ]\n",
        "\n",
        "    inputs = Input(shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = down_stack(inputs)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        concat = Concatenate()\n",
        "        x = concat([x, skip])\n",
        "\n",
        "    # This is the last layer of the model\n",
        "    last = Conv2DTranspose(output_channels, 3, strides=2, padding='same', activation='sigmoid')  #64x64 -> 128x128\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=x)"
      ],
      "metadata": {
        "id": "Uml6n67h4OUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLi17lixu2PR"
      },
      "outputs": [],
      "source": [
        "def unet_model(output_channels):\n",
        "    IMG_HEIGHT = X_train.shape[1]\n",
        "    IMG_WIDTH  = X_train.shape[2]\n",
        "    IMG_CHANNELS = X_train.shape[3]\n",
        "\n",
        "    base_model = MobileNetV2(input_shape=[256, 256, 3], include_top=False)\n",
        "\n",
        "\n",
        "    # Use the activations of these layers\n",
        "    layer_names = [\n",
        "        'block_1_expand_relu',   # 64x64\n",
        "        'block_3_expand_relu',   # 32x32\n",
        "        'block_6_expand_relu',   # 16x16\n",
        "        'block_13_expand_relu',  # 8x8\n",
        "        'block_16_project',      # 4x4\n",
        "    ]\n",
        "\n",
        "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "    # Create the feature extraction model\n",
        "    down_stack = Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "    down_stack.trainable = False\n",
        "\n",
        "    up_stack = [\n",
        "        pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "        pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "        pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "        pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "    ]\n",
        "\n",
        "    inputs = Input(shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = down_stack(inputs)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        concat = Concatenate()\n",
        "        x = concat([x, skip])\n",
        "\n",
        "    # This is the last layer of the model\n",
        "    last = Conv2DTranspose(output_channels, 3, strides=2, padding='same', activation='sigmoid')  #64x64 -> 128x128\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVv4HFyhw0FH"
      },
      "outputs": [],
      "source": [
        "def create_mask(pred_mask):\n",
        "    pred_mask = tf.sigmoid(pred_mask)\n",
        "    pred_mask = pred_mask[..., tf.newaxis]\n",
        "    return pred_mask[0]\n",
        "\n",
        "\n",
        "def show_predictions(epoch, dataset=None, num=50):\n",
        "  if dataset:\n",
        "\n",
        "    for image, mask in dataset.take(num):\n",
        "        pred_mask = model.predict(image)\n",
        "        pred_mask_bin = tf.cast(create_mask(pred_mask) > 0.5, tf.int32)\n",
        "        mask_bin = tf.cast(mask[0], tf.int32)\n",
        "        iou_val = np.sum(pred_mask_bin * mask_bin) / (np.sum(pred_mask_bin) + np.sum(mask_bin) - np.sum(pred_mask_bin * mask_bin) + 1e-7)\n",
        "        print(f\"IOU for this image: {iou_val:.4f}\")\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.subplot(231)\n",
        "        plt.title('Testing Image')\n",
        "        plt.imshow(image[0], cmap='gray')\n",
        "        plt.subplot(232)\n",
        "        plt.title('Ground Truth')\n",
        "        plt.imshow(mask[0], cmap='gray')\n",
        "        plt.subplot(233)\n",
        "        plt.title('Prediction on test image')\n",
        "        plt.imshow(create_mask(pred_mask), cmap='gray')\n",
        "\n",
        "        plt.savefig(f\"results/mask_{str(ii)}.png\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "  else:\n",
        "      fig = plt.figure(figsize=(12, 12))\n",
        "      fig.suptitle(f\"\\n Epoch: {str(epoch)}\\n\", fontsize=16)\n",
        "\n",
        "      plt.subplot(331)\n",
        "      plt.title('Testing Image')\n",
        "      plt.imshow(train_images[num], cmap='gray')\n",
        "      plt.subplot(332)\n",
        "      plt.title('Ground Truth')\n",
        "      plt.imshow(train_masks[num], cmap='gray')\n",
        "      plt.subplot(333)\n",
        "      plt.title('Prediction on test image')\n",
        "      plt.imshow(create_mask(model.predict(train_images[num][tf.newaxis, ...]))[:,:,0], cmap='gray')\n",
        "\n",
        "      plt.subplot(334)\n",
        "      plt.imshow(train_images[num+16], cmap='gray')\n",
        "      plt.subplot(335)\n",
        "      plt.imshow(train_masks[num+16], cmap='gray')\n",
        "      plt.subplot(336)\n",
        "      plt.imshow(create_mask(model.predict(train_images[num+16][tf.newaxis, ...]))[:,:,0], cmap='gray')\n",
        "      plt.subplot(337)\n",
        "      plt.imshow(train_images[num+14], cmap='gray')\n",
        "      plt.subplot(338)\n",
        "      plt.imshow(train_masks[num+14], cmap='gray')\n",
        "      plt.subplot(339)\n",
        "      plt.imshow(create_mask(model.predict(train_images[num+14][tf.newaxis, ...]))[:,:,0], cmap='gray')\n",
        "\n",
        "      #plt.savefig(f\"results/mask_{str(num+100)}_{str(epoch)}.png\")\n",
        "\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_channels = 1  # یا تعداد کانال‌های خروجی مورد نظر را اینجا مشخص کنید\n",
        "model = unet_model1(output_channels)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', iou_metric])\n"
      ],
      "metadata": {
        "id": "AY3lsTAxj7aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168842de-0528-4e00-edbb-b6ef0462aff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3954321839.py:6: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=[IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS], include_top=False, weights='imagenet')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "52NH26ERw45p",
        "outputId": "654eb0cd-a74d-42b4-c3dc-49dd1a4fbcf3"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "VAL_SUBSPLITS = 5\n",
        "BATCH_SIZE = 16\n",
        "VALIDATION_STEPS = len(X_val)//BATCH_SIZE//VAL_SUBSPLITS\n",
        "STEPS_PER_EPOCH = len(X_train)//BATCH_SIZE\n",
        "# sample_image = train_images[0]\n",
        "# sample_mask = train_masks[0]\n",
        "\n",
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        show_predictions(epoch)\n",
        "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
        "\n",
        "model_history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
        "                           batch_size = BATCH_SIZE,\n",
        "                          verbose=1,\n",
        "                          #steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          #validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=(X_val, y_val),\n",
        "                          callbacks=[DisplayCallback()]\n",
        "                          )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Plot Loss ----\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(model_history.history['loss'], label='Training Loss')\n",
        "plt.plot(model_history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---- Plot Accuracy ----\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(model_history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WD65gOG13fI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# ذخیره history بعد از آموزش\n",
        "with open('/content/drive/MyDrive/seg_mod_1/training_history.json', 'w') as f:\n",
        "    json.dump(model_history.history, f)\n"
      ],
      "metadata": {
        "id": "4Zb-ztuxfku8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# بارگذاری history\n",
        "with open('/content/drive/MyDrive/seg_mod_1/training_history.json', 'r') as f:\n",
        "    history = json.load(f)\n",
        "\n",
        "# رسم تغییرات IOU\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history['iou_metric'], label='Training IOU')\n",
        "plt.plot(history['val_iou_metric'], label='Validation IOU')\n",
        "plt.title('IOU Metric Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IOU')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sRRwjnrH3i7H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}